机器学习（Machine Learning）是从数据中自动分析获得模型，并利用模型对未知数据进行预测。

机器学习工作流程可总结如下：
- 1）获取数据
- 2）数据基本处理
- 3）特征工程
- 4）机器学习(模型训练)
- 5）模型评估
- 6）结果达到要求，形成知识；

没有达到要求，重新上面步骤。

在机器学习之前，我们需要先了解一下数据预处理及基础统计分析方法。

# 一、数据探索

## 1.1、统计量分析

### 1.1.1）集中趋势的度量
从一组数字中我们可以学到什么？

在机器学习（和数学）中，通常存在三个我们感兴趣的值：
- 平均值-平均值
- 中位数-中点值
- 众数-最常见的值

示例：我们已经注册了13辆车的速度：
```text
speed = [99,86,87,88,111,86,103,87,94,78,77,85,86]
```

平均值，中间值或最常见的速度值是多少？

#### 均值/平均值 Mean
要计算平均值，请找到所有值的总和，然后将总和除以值的数量：
```text
(99+86+87+88+111+86+103+87+94+78+77+85+86) / 13 = 89.77
```

NumPy模块为此提供了一种方法。在我们的NumPy教程中了解有关NumPy模块的信息。

例如：

使用NumPy中的mean()方法查找平均速度：
```text
import numpy

speed = [99, 86, 87, 88, 111, 86, 103, 87, 94, 78, 77, 85, 86]
x = numpy.mean(speed)
print(x)
```
output:
```text
89.76923076923077
```

#### 中值/中位数 Median
中值是对所有值进行排序后的中间值：
```text
77, 78, 85, 86, 86, 86, 87, 87, 88, 94, 99, 103, 111
```

在找到中位数之前，对数字进行排序很重要。

NumPy模块为此提供了一种方法：

例如：

使用NumPy中的median()方法查找中间值：
```text
import numpy

speed = [99, 86, 87, 88, 111, 86, 103, 87, 94, 78, 77, 85, 86]
x = numpy.median(speed)
print(x)
```
output:
```text
87.0
```

如果中间有两个数字，则将这些数字之和除以2。
```text
77, 78, 85, 86, 86, 86, 87, 87, 94, 98, 99, 103

(86 + 87) / 2 = 86.5
```

例如：

使用NumPy模块：
```text
import numpy

speed = [99, 86, 87, 88, 86, 103, 87, 94, 78, 77, 85, 86]
x = numpy.median(speed)
print(x)
```
output:
```text
86.5
```

#### 众数 Mode
众数值是出现次数最多的值：
```text
99,86, 87, 88, 111,86, 103, 87, 94, 78, 77, 85,86 = 86
```

SciPy模块为此提供了一种方法。在我们的SciPy教程中了解有关SciPy模块的信息。

例如：

使用SciPy中的mode()方法查找出现次数最多的数字：
```text
from scipy import stats

speed = [99, 86, 87, 88, 111, 86, 103, 87, 94, 78, 77, 85, 86]
x = stats.mode(speed, keepdims=True)
print(x)
```
output:
```text
ModeResult(mode=array([86]), count=array([3]))
```

平均值，中位数和众数是机器学习中经常使用的技术，因此了解它们背后的概念很重要。

### 1.1.2）离散趋势的度量

#### 极差(Range)
极差(Range)：全组内数据最大值与最小值的差。
```text
import numpy

speed = [86, 87, 88, 86, 87, 85, 86]
x = numpy.max(speed) - numpy.min(speed)
print(x)
```
output:
```text
3
```

极差易受极端值影响，且不能反映数据的中间分布情况。

#### 标准差(Standard Deviation)/方差(Variance)
**1）什么是标准偏差?**

标准偏差是一个数字，描述值的分散程度。

低标准偏差意味着大多数数字接近均值（平均值）。

高标准偏差表示这些值分布在更宽的范围内。

示例：这次我们已经注册了7辆车的速度：
```text
speed = [86, 87, 88, 86, 87, 85, 86]
```

标准偏差为：0.9。

意味着大多数值在平均值的0.9范围内，即86.4 - 0.9 ~ 86.4 + 0.9之间。

让我们对范围更广的数字进行选择：
```text
speed = [32, 111, 138, 28, 59, 77, 97]
```

标准偏差为：37.85。

这意味着大多数值在平均值的37.85范围内，即77.4。

如您所见，较高的标准偏差表示这些值分布在较宽的范围内。

NumPy模块有一种计算标准偏差的方法：

例如：

使用NumPy中的std()方法查找标准偏差：
```text
import numpy

speed = [86, 87, 88, 86, 87, 85, 86]
x = numpy.std(speed)
print(x)
```
output:
```text
0.9035079029052513
```

例如：
```text
import numpy

speed = [32, 111, 138, 28, 59, 77, 97]
x = numpy.std(speed)
print(x)
```
output:
```text
37.84501153334721
```

**2）方差(Variance)**

方差是另一个数字，指示值的分散程度。

实际上，如果采用方差的平方根，则会得到标准偏差！

或反之，如果将标准偏差乘以自身，就可以得到方差！

要计算方差，必须执行以下操作：

2.1）找到均值：
```text
(32+111+138+28+59+77+97) / 7 = 77.4
```

2.2）对于每个值：找到与平均值的差：
```text
32 - 77.4 = -45.4
111 - 77.4 =  33.6
138 - 77.4 =  60.6
28 - 77.4 = -49.4
59 - 77.4 = -18.4
77 - 77.4 = - 0.4
97 - 77.4 =  19.6
```

2.3）对于每个差异：找到平方值：
```text
(-45.4)2 = 2061.16
(33.6)2 = 1128.96
(60.6)2 = 3672.36
(-49.4)2 = 2440.36
(-18.4)2 =  338.56
(- 0.4)2 =    0.16
(19.6)2 =  384.16
```

2.4）方差是这些平方差的平均值：
```text
(2061.16+1128.96+3672.36+2440.36+338.56+0.16+384.16) / 7 = 1432.2
```

幸运的是，NumPy有一种计算方差的方法：

例如：

使用NumPy中的var()方法来查找差异：
```text
import numpy

speed = [32, 111, 138, 28, 59, 77, 97]
x = numpy.var(speed)
print(x)
```
output:
```text
1432.2448979591834
```

**3）标准偏差(Standard Deviation)**
例如：

使用NumPy中的std()方法查找标准偏差：
```text
import numpy

speed = [32, 111, 138, 28, 59, 77, 97]
x = numpy.std(speed)
print(x)
```
output:
```text
37.84501153334721
```

```text
import numpy as np

arr = np.array([1, 2, 3, 4])
print("variance of [1,2,3,4]:", np.var(arr))
print("sqrt of variance [1,2,3,4]:", np.sqrt(np.var(arr)))
print("standard deviation: np.std()", np.std(arr))
```
output:
```text
variance of [1,2,3,4]: 1.25
sqrt of variance [1,2,3,4]: 1.118033988749895
standard deviation: np.std() 1.118033988749895
```

**4）符号**

标准偏差通常用符号Sigma：σ

方差通常用符号Sigma Square: σ2

标准偏差和方差是机器学习中经常使用的术语，因此了解如何获取它们以及它们背后的概念非常重要。

#### 百分位数

在统计数据中使用百分位数可为您提供一个数字，该数字描述了给定百分比值低于的值。

示例：假设我们有一个街道上所有人口的年龄数组。
```text
ages = [5,31,43,48,50,41,7,11,15,39,80,82,32,2,8,6,25,36,27,61,31]
```

什么是75.百分位数？答案是43，这意味着75％的人是43岁或以下。

NumPy模块具有一种用于找到指定百分位数的方法：

例如：

使用NumPy中的percentile()方法查找百分位数：
```text
import numpy

ages = [5, 31, 43, 48, 50, 41, 7, 11, 15, 39, 80, 82, 32, 2, 8, 6, 25, 36, 27, 61, 31]
x = numpy.percentile(ages, 75)
print(x)
```
output:
```text
43.0
```

例如：

90％的人口年龄是多少岁？
```text
import numpy

ages = [5, 31, 43, 48, 50, 41, 7, 11, 15, 39, 80, 82, 32, 2, 8, 6, 25, 36, 27, 61, 31]
x = numpy.percentile(ages, 90)
print(x)
```
output:
```text
61.0
```

```text
import numpy as np

a = np.array([[10, 7, 4], [3, 2, 1]])
print('我们的数组是：')
print(a)

print('调用 percentile() 函数：')
# 50% 的分位数，就是 a 里排序之后的中位数
print(np.percentile(a, 50))

# axis 为 0，在纵列上求
print(np.percentile(a, 50, axis=0))

# axis 为 1，在横行上求
print(np.percentile(a, 50, axis=1))

# 保持维度不变
print(np.percentile(a, 50, axis=1, keepdims=True))
```
output:
```text
我们的数组是：
[[10  7  4] [ 3  2  1]]
调用 percentile() 函数：
3.5
[6.5 4.5 2.5]
[7. 2.]
[[7.] [2.]]
```

#### 变异系数

极差，方差，标准差都是有计量单位的。

如果要比较不同数据的离散程度，需要用变异系数。

变异系数是一组数据中的极差、四分位差或标准差等离散指标与算术平均数的比率。

```text
import numpy as np
import scipy.stats as st

# 学生体重测试数据
weights = np.array([75.0, 64.0, 47.4, 66.9, 62.2, 62.2, 58.7, 63.5,
                    66.6, 64.0, 57.0, 69.0, 56.9, 50.0, 72.0])

# 极差
R_weights = np.max(weights) - np.min(weights)
print('体重数据的极差：%0.2f' % R_weights)
R_weights1 = np.ptp(weights)
print('体重数据的极差：%0.2f' % R_weights1)

# 均值的两种方式
w_mean = np.mean(weights)
w_mean1 = weights.mean()
# 限定范围的数据求均值，排除离群值（只统计60~70之间的数据），注意不要想当然的排除。
limitedMean = st.tmean(weights, (60, 70))

# 对数据进行排列
sorted_weight = sorted(weights, reverse=True)

# 求中位数（在对称分布比如正态分布和T分布，均值和中位数很接近；在偏态分布的情况下，比如F分布，均值和中位数相差较大）
median_weight = np.median(weights)
# 求分位数
quantiles = np.quantile(weights, [0.1, 0.2, 0.4, 0.6, 0.8, 1])
print('学生体重的[10%, 20%, 40%, 60%, 80%, 100%]分位数：', quantiles)

# 方差和方差的无偏估计
# 总体方差，即方差的有偏估计，这里的样本误差其实指的是母体误差，也就是说除以的是n而非（n-1）
v = np.var(weights)
# 样本方差，即方差的无偏估计，也就是真正意义上的样本误差，除以的是(n-1)
v_unb = st.tvar(weights)
print('体重数据方差的估计为：%0.2f,无偏估计为：%0.2f' % (v, v_unb))

# 标准差，同样存在有偏和无偏估计
s = np.std(weights)  # 有偏的标准差，同上
s_unb = st.tstd(weights)  # 无偏的标准差，同上
print('体重数据标准差的估计为：%0.2f,无偏估计为：%0.2f' % (s, s_unb))

# 变异系数(标准差的无偏估计除以均值再乘100)，无量纲，百分数表示
cv = s_unb / w_mean * 100
print('体重数据的变异系数为：', np.round(cv, 2), '%')
```
output:
```text
体重数据的极差：27.60
体重数据的极差：27.60
学生体重的[10%, 20%, 40%, 60%, 80%, 100%]分位数： [52.76 56.98 62.2  64.   67.32 75.  ]
体重数据方差的估计为：52.71,无偏估计为：56.47
体重数据标准差的估计为：7.26,无偏估计为：7.51
体重数据的变异系数为： 12.05 %
```

### 1.1.3）描述性统计函数
python Dataframe.describe()方法
```text
import pandas as pd

data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],
        'Gender': ['Female', 'Male', 'Male', 'Male', 'Female'],
        'Age': [25, 30, 35, 40, 45],
        'City': ['Beijing', 'Shanghai', 'Beijing', 'Shanghai', 'Beijing'],
        'Salary': [5000, 6000, 7000, 8000, 9000]}
df = pd.DataFrame(data)
pt1 = df.describe()
print(pt1)
```
output:（describe方法会忽略掉非数字字段）
```text
             Age      Salary
count   5.000000     5.00000
mean   35.000000  7000.00000
std     7.905694  1581.13883
min    25.000000  5000.00000
25%    30.000000  6000.00000
50%    35.000000  7000.00000
75%    40.000000  8000.00000
max    45.000000  9000.00000
```

## 1.2、数据透视表
数据透视表是一种分类汇总数据的方法。

在 Python 中，我们可以使用 Pandas 库中的pivot_table 函数来创建和操作数据透视表，
相比Excel，Python可以更多更快的处理数据。

首先，介绍一下pivot_table 函数：
```text
pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All')
```
看一下参数主要的作用：
```text
data:DataFrame

values:被计算的数据项，设定需要被聚合操作的列（需要显示的列）

index:每个pivot_table必须拥有一个index,必选参数，设定数据的行索引，可以设置多层索引，多次索引时按照需求确定索引顺序。

columns:必选参数，设定列索引，用来显示字符型数据，和fill_value搭配使用。

aggfunc:聚合函数， pivot_table后新dataframe的值都会通过aggfunc进行运算。默认numpy.mean求平均。

fill_values:填充NA值（设定缺失值）。默认不填充，可以指定。

margins：添加行列的总计，默认FALSE不显示。TRUE显示。

dropna：如果整行都为NA值，则进行丢弃，默认TRUE丢弃。FALSE时，被保留。

margins_name：margins = True 时，设定margins 行/列的名称。‘all’ 默认值
```

1）按照不同类进行计数统计

为了结算每个城市不同性别的平均收入，我们可以利用pivot_table函数进行统计：
```text
import pandas as pd

data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],
        'Gender': ['Female', 'Male', 'Male', 'Male', 'Female'],
        'Age': [25, 30, 35, 40, 45],
        'City': ['Beijing', 'Shanghai', 'Beijing', 'Shanghai', 'Beijing'],
        'Salary': [5000, 6000, 7000, 8000, 9000]}
df = pd.DataFrame(data)
pt1 = df.pivot_table(index='City', columns='Gender', values='Salary', aggfunc='mean')
print(pt1)
```
output:
```text
Gender    Female    Male
City                    
Beijing   7000.0  7000.0
Shanghai     NaN  7000.0
```
2）如果要知道每个城市不同性别的总收入，只需要设定aggfunc函数为sum进行求和：
```text
import pandas as pd

data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],
        'Gender': ['Female', 'Male', 'Male', 'Male', 'Female'],
        'Age': [25, 30, 35, 40, 45],
        'City': ['Beijing', 'Shanghai', 'Beijing', 'Shanghai', 'Beijing'],
        'Salary': [5000, 6000, 7000, 8000, 9000]}
df = pd.DataFrame(data)
pt1 = df.pivot_table(index='City', columns='Gender', values='Salary', aggfunc='sum')
print(pt1)
```
output:
```text
Gender     Female     Male
City                      
Beijing   14000.0   7000.0
Shanghai      NaN  14000.0
```

3）可以看到这两个数据透视表是有缺失值的，pivot_table有一个参数fill_value，就是用来填充这些缺失值的。
```text
import pandas as pd

data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],
        'Gender': ['Female', 'Male', 'Male', 'Male', 'Female'],
        'Age': [25, 30, 35, 40, 45],
        'City': ['Beijing', 'Shanghai', 'Beijing', 'Shanghai', 'Beijing'],
        'Salary': [5000, 6000, 7000, 8000, 9000]}
df = pd.DataFrame(data)
pt1 = df.pivot_table(index='City', columns='Gender', values='Salary', fill_value=0, aggfunc='sum')
print(pt1)
```
output:
```text
Gender     Female     Male
City                      
Beijing   14000.0   7000.0
Shanghai      0     14000.0
```
4）pivot_table方法还支持对透视表进行统计计算，而且会新建一个列来存放计算结果。

这个统计需要用到以下两个参数：
- margins，设定是否添加汇总列，一般设置为True。
- margins_name，汇总列的名称。

```text
import pandas as pd

data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],
        'Gender': ['Female', 'Male', 'Male', 'Male', 'Female'],
        'Age': [25, 30, 35, 40, 45],
        'City': ['Beijing', 'Shanghai', 'Beijing', 'Shanghai', 'Beijing'],
        'Salary': [5000, 6000, 7000, 8000, 9000]}
df = pd.DataFrame(data)
pt1 = df.pivot_table(index='City', columns='Gender', values='Salary',
                     fill_value=0, aggfunc='sum', margins=True, margins_name="汇总")
print(pt1)
```
output:
```text
Gender    Female   Male     汇总
City                          
Beijing    14000   7000  21000
Shanghai       0  14000  14000
汇总         14000  21000  35000
```

5）筛选数据透视表中的数据

（1）仅保留汇总列的数据。
```text
import pandas as pd

data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],
        'Gender': ['Female', 'Male', 'Male', 'Male', 'Female'],
        'Age': [25, 30, 35, 40, 45],
        'City': ['Beijing', 'Shanghai', 'Beijing', 'Shanghai', 'Beijing'],
        'Salary': [5000, 6000, 7000, 8000, 9000]}
df = pd.DataFrame(data)
pt1 = df.pivot_table(index='City', columns='Gender', values='Salary',
                     fill_value=0, aggfunc='sum', margins=True, margins_name="汇总")
print(pt1["汇总"])
```
output:
```text
City
Beijing     21000
Shanghai    14000
汇总          35000
Name: 汇总, dtype: int64
```

（2）获取男性和女性的汇总数据。
```text
import pandas as pd

data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],
        'Gender': ['Female', 'Male', 'Male', 'Male', 'Female'],
        'Age': [25, 30, 35, 40, 45],
        'City': ['Beijing', 'Shanghai', 'Beijing', 'Shanghai', 'Beijing'],
        'Salary': [5000, 6000, 7000, 8000, 9000]}
df = pd.DataFrame(data)
pt1 = df.pivot_table(index='City', columns='Gender', values='Salary',
                     fill_value=0, aggfunc='sum', margins=True, margins_name="汇总")
print(pt1[["Female", "Male"]])
```
output:
```text
Gender    Female   Male
City                   
Beijing    14000   7000
Shanghai       0  14000
汇总         14000  21000
```

6）使用字段列表对数据透视表中的数据进行排序
```text
import pandas as pd

data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],
        'Gender': ['Female', 'Male', 'Male', 'Male', 'Female'],
        'Age': [25, 30, 35, 40, 45],
        'City': ['Beijing', 'Shanghai', 'Beijing', 'Shanghai', 'Beijing'],
        'Salary': [5000, 6000, 7000, 8000, 9000]}
df = pd.DataFrame(data)
pt1 = df.pivot_table(index='City', columns='Gender', values='Salary',
                     fill_value=0, aggfunc='sum', margins=True, margins_name="汇总")
print("=================== 排序前 ====================")
print(pt1)
print("=============== 按照 汇总 排序后 ================")
print(pt1.sort_values(by="汇总"))
```
output:
```text
=================== 排序前 ====================
Gender    Female   Male     汇总
City                          
Beijing    14000   7000  21000
Shanghai       0  14000  14000
汇总         14000  21000  35000
=============== 按照 汇总 排序后 ================
Gender    Female   Male     汇总
City                          
Shanghai       0  14000  14000
Beijing    14000   7000  21000
汇总         14000  21000  35000
```

以上就是用Python构造数据透视表的内容介绍。
数据透视表是数据分析中非常重要的工具，掌握它可以让你更加高效地进行数据处理和可视化呈现。

## 1.3、相关性分析

1）散点图，散点图矩阵

2）相关系数，python Dataframe.corr方法
```text
import pandas as pd

df = pd.read_csv("data.csv")
print("==========数据信息：==========")
print(df.info())
print("==========数据相关性分析：==========")
print(df.corr())
```
output:
```text
==========数据信息：==========
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 169 entries, 0 to 168
Data columns (total 4 columns):
 #   Column    Non-Null Count  Dtype  
---  ------    --------------  -----  
 0   Duration  169 non-null    int64  
 1   Pulse     169 non-null    int64  
 2   Maxpulse  169 non-null    int64  
 3   Calories  164 non-null    float64
dtypes: float64(1), int64(3)
memory usage: 5.4 KB
None
==========数据相关性分析：==========
          Duration     Pulse  Maxpulse  Calories
Duration  1.000000 -0.155408  0.009403  0.922717
Pulse    -0.155408  1.000000  0.786535  0.025121
Maxpulse  0.009403  0.786535  1.000000  0.203813
Calories  0.922717  0.025121  0.203813  1.000000
```
根据结果可以看出，Calories字段与Duration字段相关性大，达到0.922717。

# 二、数据预处理
数据分析与预处理是数据科学领域中至关重要的两个环节。

【数据分析】是指对收集到的数据进行分析和挖掘，以发现数据中隐藏的模式、规律和关联，帮助做出准确的决策。

而【数据预处理】则是在数据分析之前的一个阶段，主要是对原始数据进行清洗、转换和集成，以便后续的分析处理。

【数据预处理】主要包括以下几个方面：

第一，【数据清洗（Data Cleaning）】是指去除数据中的无效信息、错误信息和冲突信息，填充缺失值，处理异常值等操作，以确保数据的质量和完整性。

第二，【数据集成（Data Integration）】是指将多个数据源的数据整合到一个统一的数据存储中，消除数据之间的冲突和重复，以便进行后续分析。

第三，【数据变换（Data Transformation）】是指对数据进行规范化、标准化、离散化、归一化等操作，以适应不同的分析模型和算法的需求。

第四，【数据规约（Data Reduction）】是指通过属性选择、数据聚集、维度规约等方法减少数据集的复杂度和维度，提高数据处理的效率和速度。

通过数据预处理，可以使数据更加清晰、准确，为后续的数据分析和建模提供可靠的数据基础。

数据分析则是在数据预处理之后，利用各种统计分析、机器学习算法等方法对数据进行深入挖掘和分析，从而得出有意义的结论和见解，帮助做出科学的决策和预测。

总之，数据分析与预处理是数据科学领域中不可或缺的两个环节，是从原始数据中提取有用信息、发现价值和洞察的重要步骤，对于推动企业发展、提升竞争力具有重要意义。

## 2.1、数据清洗

### 2.1.1）缺失值处理

在 Pandas 中，处理空值（缺失值）是数据清洗中的一个重要步骤。以下是一些常见的处理空值的方法：

**1）检测空值：**

使用 isnull() 或 isna() 方法来检测空值。
这将返回一个布尔值的 DataFrame，其中 True 表示对应位置是空值。
```text
import pandas as pd

# 创建一个示例 DataFrame
data = {'Name': ['Alice', 'Bob', None, 'Charlie'],
        'Age': [25, 30, None, 35]}
df = pd.DataFrame(data)
# 检测空值
print(df.isnull())
```
output:
```text
    Name    Age
0  False  False
1  False  False
2   True   True
3  False  False
```

**2）删除空值：**

使用 dropna() 方法删除包含空值的行或列。

dropna() 默认删除包含任何空值的行，你也可以通过设置参数来调整删除的方式。
```text
import pandas as pd

# 创建一个示例 DataFrame
data = {'Name': ['Alice', 'Bob', None, 'Charlie'],
        'Age': [25, 30, None, 35]}
df = pd.DataFrame(data)
print("===========判断空值===========")
# 检测空值
print(df.isnull())
print("===========删除空列===========")
# 删除包含空值的行
df_cleaned_c = df.dropna()
print(df_cleaned_c)
```
output:
```text
===========判断空值===========
    Name    Age
0  False  False
1  False  False
2   True   True
3  False  False
===========删除空列===========
      Name   Age
0    Alice  25.0
1      Bob  30.0
3  Charlie  35.0
```

**3）填充空值：**

使用 fillna() 方法填充空值。你可以选择用特定的值、均值、中位数等来填充。
```text
import pandas as pd

# 创建一个示例 DataFrame
data = {'Name': ['Alice', 'Bob', None, 'Charlie'],
        'Age': [25, 30, None, 35]}
df = pd.DataFrame(data)
print("===========判断空值===========")
# 检测空值
print(df.isnull())

# 用特定值填充空值
df_filled = df.fillna(value=0)
# 用均值填充空值
mean_age = df['Age'].mean()
print("===========用Age列的平均值填充Age列的空值===========")
print(df.fillna(value={'Age': mean_age}))
```
output:
```text
===========判断空值===========
    Name    Age
0  False  False
1  False  False
2   True   True
3  False  False
===========用Age列的平均值填充Age列的空值===========
      Name   Age
0    Alice  25.0
1      Bob  30.0
2     None  30.0
3  Charlie  35.0
```

**4）插值填充：**

使用 interpolate() 方法进行插值填充，根据相邻的值推断空值。这对于时间序列数据特别有用。
```text
import pandas as pd

# 创建一个示例 DataFrame
data = {'Name': ['Alice', 'Bob', None, 'Charlie'],
        'Age': [25, 30, None, 35]}
df = pd.DataFrame(data)
print("===========判断空值===========")
# 检测空值
print(df.isnull())
print("===========使用插值填充空值===========")
# 使用插值填充空值
df_interpolated = df.interpolate()
print(df_interpolated)
```
output:
```text
===========判断空值===========
    Name    Age
0  False  False
1  False  False
2   True   True
3  False  False
===========使用插值填充空值===========
      Name   Age
0    Alice  25.0
1      Bob  30.0
2     None  32.5
3  Charlie  35.0
```

**5）替换空值：**

使用 replace() 方法将空值替换为指定的值。
```text
import pandas as pd

# 创建一个示例 DataFrame
data = {'Name': ['Alice', 'Bob', None, 'Charlie'],
        'Age': [25, 30, None, 35]}
df = pd.DataFrame(data)
print("===========判断空值===========")
# 检测空值
print(df.isnull())
print("===========将空值替换为 -1===========")
# 将空值替换为 -1
df_replaced = df.replace(to_replace=pd.NA, value=-1)
print(df_replaced)
```
output:
```text
===========判断空值===========
    Name    Age
0  False  False
1  False  False
2   True   True
3  False  False
===========将空值替换为 -1===========
      Name   Age
0    Alice  25.0
1      Bob  30.0
2       -1   NaN
3  Charlie  35.0
```

这些是一些处理 Pandas DataFrame 中空值的基本方法。
具体选择哪种方法取决于你的数据特点和分析需求。
在数据清洗的过程中，根据具体情况选择适当的处理方式以确保数据质量。

### 2.1.2）异常值处理

删除

缺失处理

平均值修正

不处理

## 2.2、数据标准化

数据标准化（Normalization）是指：将数据按照一定的比例进行缩放，使其落入一个特定的小区间。

**为什么要进行数据标准化呢？**
去除数据的单位限制，将其转化为无量纲的纯数值，便于不同量级、不同单位或不同范围的数据转化为统一的标准数值，以便进行比较分析和加权。

常见的数据标准化方法有以下6种：
### （1）Min-Max标准化
Min-Max标准化是指对原始数据进行线性变换，将值映射到[0,1]之间，数据分布不变。

Min-Max标准化的计算公式为：
```text
x′= (x−min(x))/(max(x)−min(x))
```
公式中，x为原始数据中的一个数据，max(x)表示原始数据中的最大值，min(x)表示原始数据中的最大值。
### （2）Z-Score标准化
Z-Score标准化是指基于原始数据的均值和标准差来进行数据的标准化，公式如下：
```text
x′= (x−μ)/σ
```
公式中，x为原始数据中的一个数据，μ表示原始数据的均值，σ表示原始数据的标准差。
### （3）小数定标标准化
小数定标标准化是通过移动数据的小数位数，将数据映射到[-1,1]区间上，移动的小数位数取决于数据绝对值的最大值。

例如，一组数据为[-10,12,-234,78,-987,789],其中绝对值最大为-987，即改组数据所有数值小数点移动三位即可得到标准化后的数据。

小数定标标准化公式如下：
```text
x′=x/(10^j)  # x 除以10的j次方
```
式中j表示小数点移动位数。

### （4）均值归一化法
均值归一化是指通过原始数据中的均值、最大值和最小值来进行数据的标准化。
其计算公式如下：
```text
x′= (x−μ)/(max(x)−min(x))
```
公式中，x为原始数据中的一个数据，μ表示原始数据的均值。

### （5）向量归一化
向量归一化是指：通过原始数据中的每个值除以该特征所有数据之和来进行数据的标准化。
其计算公式如下：
```text
x′= x / ∑ (i=1~n) x
```
公式中，分母表示的是原始数据的该特征所有数据之和。

### （6）指数转换
指数转换是指：通过对原始数据的值进行相应的指数函数进行数据的标准化。

指数转换常见的函数方法有lg函数、Softmax函数和Sigmoid函数。

此处我们只介绍lg函数的标准化处理，
因为当数据的值过大时Softmax函数和Sigmoid函数标准化并不完全适用(虽然不详细介绍但依旧提供手写代码实现，点击此处即可前往全部代码链接)，
熟悉神经网络或深度学习的朋友知道上述两种函数常用作激活函数。

lg函数对应的标准化计算公式如下：
```text
x′ = lg(x) / lg(max(x))
```

## 2.3、属性规约

【属性规约】通过属性合并创建新属性维数，或者通过直接删除不相关的属性来减少数据维数，从而提高数据挖掘的效率，降低计算成本。

属性规约常见的方式如下表所示：

| 属性规约方法	 | 方法描述                                                                  |
|---------|-----------------------------------------------------------------------|
| 合并属性	   | 将一些旧属性合并为新属性                                                          |
| 逐步向前选择	 | 从一个空属性集开始，每次从原来属性集合中选择一个当前最优的属性添加到当前属性子集中，直到无法选出最优属性或满足一定阈值约束为止       |
| 逐步向后删除	 | 从全属性集开始，每次从当前属性子集中选择一个当前最差属性并将其从当前属性子集中消去，与上面相反                       |
| 决策树归纳	  | 利用决策树的归纳方法对初始数据进行分类归纳学习，获得一个初始决策树，所有没有出现在决策树上的属性均可认为是无关属性，因此可以将这些属性删除 |
| 主成分分析	  | 用较少的变量去解释原数据中的大部分变量，将许多相关性很高的变量转化成彼此相互独立或不相关变量                        |


# 三、利用python做基础统计分析

见python Pandas内容

## 3.1、数据获取（可用数据集）：
1、Kaggle&天池（大数据竞赛平台）；

2、UCI数据集网站（包含多领域数据）；

3、scikit-learn网址（适合学习阶段）

## 3.2、python常用的工具包：（即用即查）
数据分析工具：numpy ；scipy ；pandas

数据可视化工具：matplotlib

数据挖掘与建模工具：scikit-learn；TensorFlow

官方主页：NumPy 官方主页：SciPy 官方主页：Matplotlib — Visualization with Python

官方主页：scikit-learn: machine learning in Python — scikit-learn 0.16.1 documentation官方主页：pandas - Python Data Analysis Library

## 3.3、简单数据分类：
1、定类数据；

2、定序数据；

3、定距数据（间隔）：可以界定数据大小同时，可测定差值，但无绝对零点，乘除无意义。例如温度。

4、定比数据（比率）：可以界定数据大小，可测定差值，有绝对零点，乘除有意义，最常见的数值型数据。

## 3.4、基本的描述性分析
### 3.4.1）数据预览
```text
df=pd.read_csv("D:/Users/DXX/Desktop/dxx.code/Python学习/HR_comma_sep.csv")
df.info()
df.describe()
df.sample(n=10)  # 抽样个数
df.sample(frac=0.0005)  # 抽样百分比
```
info()——用于获取 DataFrame 的简要摘要，以便快速浏览数据集。

describe()——用于对数据进行统计学估计，
输出行名分别为：count(行数)，mean(平均值)，std(标准差)，
min(最小值），25%(第一四分位数)，50%(第二四分位数)，75%(第三四分位数)，max(最大值)。

### 3.4.2）异常值分析——需要对数据进行单变量及整体异常值分析（具体问题具体分析）
```text
### 对每组变量进行异常值分析
# "satisfaction_level"
sl_s=df["satisfaction_level"]
sl_s.describe()
sl_s[sl_s.isnull()]
df[sl_s.isnull()]
sl_s=sl_s.dropna()   #dropna（）删除nan；fillna（）填充nan
np.histogram(sl_s.values,bins=np.arange(0.0,1.1,0.1))   #负偏

# "last_evaluation"
le_s=df["last_evaluation"]
le_s.describe()
le_s[le_s<=1]
q_low=le_s.quantile(q=0.25)
q_high=le_s.quantile(q=0.75)
q_interval=q_high-q_low
k=1.5
le_s=le_s[le_s>q_low-k*q_interval][le_s<q_high+k*q_interval]   #上下四分位数阈值范围外
le_s
np.histogram(le_s.values,bins=np.arange(0.0,1.1,0.1))   #负偏

# "number_project"
np_s=df["number_project"]
np_s.describe()
np_s.value_counts(normalize=True).sort_index()   #normalize=True——返回比例；sort_index()——按照序号排序

# "average_montly_hours"&"time_spend_company"
amh_s=df["average_montly_hours"]
amh_s.describe()
amh_s=amh_s[amh_s>amh_s.quantile(q=0.25)-1.5*(amh_s.quantile(q=0.75)-amh_s.quantile(q=0.25))][amh_s<amh_s.quantile(q=0.75)+1.5*(amh_s.quantile(q=0.75)-amh_s.quantile(q=0.25))]
np.histogram(amh_s.values,bins=10)
np.histogram(amh_s.values,bins=np.arange(amh_s.min(),amh_s.max()+10,10))   #范围——左闭右开
amh_s.value_counts(bins=np.arange(amh_s.min(),amh_s.max()+10,10))   #区间——左开右闭

# "Work_accident"&"left"&"promotion_last_5years"
wa_s=df["Work_accident"]
wa_s.describe()
wa_s.value_counts()
left_s=df["left"]
left_s.describe()
left_s.value_counts()
pl5_s=df["promotion_last_5years"]
pl5_s.describe()
pl5_s.value_counts()

# "salary" &"department"
df.salary.unique()   #查看salary的类别，包括哪些工作类型
s_s=df["salary"]
s_s.value_counts()
print(s_s.where(s_s!="nme"))
s_s.where(s_s!="nme").dropna()
df.department.unique()  #查看sales的类别，包括哪些工作类型
d_s=df["department"]
d_s.value_counts(normalize=True).sort_values()

### 整体去除数据异常值
df=df.dropna(axis=0,how="any")  #axis=0—行；how=“any”有一个空值删除；“all”全为空删除
df=df[df["last_evaluation"]<=1][df["salary"]!="nme"]
```
最后实现：将数据中的nan值、超出正常范围的取值和不正确的属性值去除。

### 3.4.3）对比分析
```text
print(df.iloc[:,[1,8]])
df.loc[:,["last_evaluation","department"]]   
# loc和iloc的区别：loc按照表格名称索引；iloc按照位置索引；
```
loc和iloc的区别：loc按照表格名称索引；iloc按照位置索引。

```text
# 分组，并计算均值
df.loc[:,["last_evaluation","department"]].groupby("department").mean()

# 分组，应用匿名函数lambda进行组内运算
df.loc[:,["average_montly_hours","department"]].groupby("department")["average_montly_hours"].apply(lambda x:x.max()-x.min())   #自定义计算极差

```
groupby()——主要的作用是进行数据的分组以及分组后地组内运算（简单的均值计算 or 指自定义匿名函数运算）。

### 3.4.4）分布分析
```text
import scipy.stats as ss
import pandas as pd
import numpy as np
ss.norm  #生成一个正态分布的对象
ss.norm.stats(moments="mvsk")  #正态分布的均值；方差；偏度；峰度；
ss.norm.pdf(0.0)  #x=0对应的概率密度函数值
ss.norm.ppf(0.9)  #取值范围在[0,1]——累积分布函数为0.9对应的x值
ss.norm.cdf(2)    #x为2时，累积分布函数的取值.（范围[0,1]）
ss.norm.cdf(2)-ss.norm.cdf(-2)  
ss.norm.rvs(size=10)  #生成10个正态分布的数据
#（其他用法同正态分布）
ss.chi2   #卡方分布
ss.t    #t分布
ss.f   #f分布  
```

## 3.5、数据简单可视化分析：matplotlib；seaborn；plotly
1、柱状图
```text
### 柱状图
# arange()生成一个指定1终点2起点和3步长的列表
plt.bar(np.arange(len(df["salary"].value_counts())),df["salary"].value_counts())  
plt.show()
```

2、直方图
```text
### 直方图
sns.displot(df["satisfaction_level"],bins=10,kde=True) #kde=True有曲线
plt.show()
```

3、箱线图
```text
###箱线图
sns.boxplot(y=df["time_spend_company"])
sns.boxplot(x=df["time_spend_company"],saturation=0.75,whis=3)  # whis上界
```

4、折线图
```text
### 折线图
sns.pointplot(x="time_spend_company",y="left",data=df)
```

5、饼图
```text
### 饼图
lbs=df["department"].value_counts().index
explodes=[0.1 if i=="sales" else 0 for i in lbs]  # 与其他类别分隔开
plt.pie(df["department"].value_counts(normalize=True),explode=explodes,labels=lbs,autopct="%1.1f%%",colors=sns.color_palette("Greens"))
```

---

# 数据分析和数据预处理的阐述

【数据分析与预处理】是数据科学领域中非常重要的两个环节，它们通常是数据处理流程中的首要步骤。

【数据分析】指的是研究和解释数据的过程，以揭示其中的规律、趋势和模式，从而为决策制定提供支持。

而【数据预处理】则是在数据分析之前对数据进行清洗、转换和组织，以便更有效地进行分析和挖掘数据中的潜在信息。

## 数据分析：

- 定义：数据分析是指利用统计学和机器学习等技术，对收集到的数据进行分析、研究和解释的过程。其目的是根据数据之间的关系和模式，为决策提供支持和指导。
- 工具：数据分析通常借助于统计分析软件（如R、Python的pandas和numpy库、SPSS等）来实现。
- 方法：常用的数据分析方法包括描述统计、推断统计、回归分析、聚类分析和关联规则挖掘等。
- 应用：数据分析广泛应用于商业分析、市场营销、金融风控、医疗诊断、社会科学研究等领域。

## 数据预处理：

- 定义：数据预处理是指在进行数据分析之前，对原始数据进行清洗、转换和整理，以去除数据中的噪声和异常值，填补缺失值，以及将数据格式转换为适合模型应用的形式。
- 工具：数据预处理通常使用数据处理工具和编程语言来进行，如Excel、Python、R等。
- 方法：主要的数据预处理方法包括数据清洗、缺失值处理、异常值处理、数据变换、特征选择和特征提取等。
- 作用：数据预处理能够提高数据质量，避免分析的误差和偏差，从而确保数据分析的准确性和可靠性。

## 数据清洗：

- 定义：数据清洗是数据预处理的重要环节之一，其目的是检测和纠正数据集中的错误、缺失、重复和不一致之处。
- 方法：数据清洗包括去除重复数据、处理缺失值、修正错误数据、处理异常值、处理不一致数据等。
- 作用：数据清洗能够提高数据的质量和完整性，避免由错误数据引起的分析结果失真。

## 特征工程：

- 定义：特征工程是数据预处理的一个重要步骤，其目的是根据数据的特点和业务需求，对数据进行特征提取、变换和选择，以构建适合模型应用的特征集合。
- 方法：特征工程包括对原始特征进行编码、缩放、转换、组合和选择，以提取更有意义且更具区分性的特征。
- 作用：良好的特征工程能够提高模型的性能和泛化能力，降低模型的过拟合风险，从而提高数据分析的准确性和可靠性。

## 数据转换：

- 定义：数据转换是数据预处理的一个步骤，其目的是对原始数据进行变换，使数据更适合进行分析和建模。
- 方法：数据转换包括对数据进行标准化、归一化、正态化、离散化等操作，以便消除不同特征之间的量纲差异或满足模型的需求。
- 作用：数据转换有助于提高数据的可解释性，降低数据分析的复杂度，提高模型的稳定性和可解释性。