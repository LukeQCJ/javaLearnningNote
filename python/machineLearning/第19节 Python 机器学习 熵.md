# Python 机器学习 熵

熵（Entropy）在机器学习中是一个重要的概念，尤其是在决策树算法和信息理论中。

【熵】用于【量化】数据集的不确定性或杂乱无章的程度。

在决策树中，熵帮助我们确定哪个特征最好地分割数据集，以便尽可能清晰地分类数据点。

熵不仅用于构建决策树，也是评估分类问题中不同模型或特征选择方法效果的有用工具。

理解和计算熵对于设计高效的机器学习模型至关重要。

## 1、熵的定义
在机器学习和信息论中，熵是一个衡量随机变量不确定性的重要概念。

它的定义源自于信息论，由克劳德·香农在1948年提出。

熵的概念被广泛应用于各种领域，包括机器学习、数据压缩、密码学等。

【熵的直观含义】是【描述一个系统的随机性或者信息的不确定性】。

在机器学习中，熵通常用于衡量数据集中的不纯度或混乱程度，尤其在决策树算法中用于特征选择和数据划分。

**熵越高，数据的不确定性就越大；熵越低，数据的不确定性就越小。**

公式代码如下，

```text
import math


def entropy(probabilities):
    """
        计算熵
        参数:
            probabilities -- 可迭代对象，包含随机变量的概率分布
        返回:
            熵的值
    """
    # 确保概率分布是有效的（总和为1）
    if not math.isclose(sum(probabilities), 1, abs_tol=1e-9):
        raise ValueError("概率之和必须等于1")
    # 计算熵
    return -sum(p * math.log2(p) for p in probabilities if p > 0)


# 示例：等概率分布
probabilities = [0.25, 0.25, 0.25, 0.25]
print("熵:", entropy(probabilities))
# 示例：非等概率分布
probabilities = [0.1, 0.4, 0.3, 0.2]
print("熵:", entropy(probabilities))
```
output:
```text
熵: 2.0
熵: 1.8464393446710154
```

## 2、熵的计算
在信息论和机器学习中，熵是衡量数据集内部不确定性（或纯度）的一个重要概念。
在决策树等算法中，熵常用来评估一个属性划分数据的有效性。

数据集的熵越低，数据的纯度越高。熵的值范围从0（完全纯净）到接近1（完全不纯净，对于均匀分布的多类情况，熵可能会超过1）。

在决策树等算法中，我们寻找可以最大化熵减少（或信息增益）的特征，以选择最佳的分割点。

```text
import numpy as np


def calculate_entropy(p):
    """
        计算熵
    参数:
        - p: 一个包含事件概率的列表或numpy数组

    返回:
        - 熵的值
    """
    p = p[p > 0]  # 过滤掉概率为0的事件，因为0*log(0)在数学上是未定义的，但按照极限是等于0的
    return -np.sum(p * np.log2(p))


# 示例：一个数据集中有3个类，每个类的概率
arr = np.array([0.5, 0.3, 0.2])
entropy = calculate_entropy(arr)
print(f"熵: {entropy}")
```
output:
```text
熵: 1.4854752972273344
```

## 3、熵的应用
在机器学习中，熵是衡量数据集或信号中不确定性或杂乱无章程度的重要指标。
它在多个领域中都有应用，特别是在决策树算法、信息增益计算、特征选择、数据压缩和数据挖掘等方面。

### 1）决策树算法

熵是决策树学习算法中重要的概念，尤其是在构造决策树时评估特征的重要性。
信息增益（即父节点熵与加权子节点熵之差）通常用于CART和ID3等算法中，以选择最佳分割特征。

### 2）特征选择

在特征选择阶段，熵可以帮助评估哪些特征对于分类或回归任务更为重要。通过计算特征的信息增益或互信息，可以识别出对目标变量有更大预测能力的特征。

### 3）数据压缩

熵还可以用于数据压缩领域，特别是在无损压缩算法中。熵编码（如霍夫曼编码）基于字符出现的概率来分配不等长的编码，以此减少编码后数据的平均长度。

### 4）数据挖掘和知识发现

熵可以用于发现数据中的模式和关联规则，尤其是在处理大型数据集时评估不同数据分割或聚类的有效性。

决策树分类器时应用熵示例如下，

```text
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

# 加载Iris数据集
iris = load_iris()
X, y = iris.data, iris.target

# 创建决策树模型，使用熵作为分裂标准
model = DecisionTreeClassifier(criterion='entropy')

# 训练模型
model.fit(X, y)

# 使用模型进行预测
predictions = model.predict(X)
print(predictions)
```
output:
```text
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
```